
Durante a Sprint 2, aprendi muita coisa, 


Atualmente, j√° conclu√≠ os dois cursos (SQL e AWS) e estou gostando bastante da trilha. Estou ansioso pela pr√≥xima sprint e pelos novos aprendizados que ela trar√° üòä.

# Certificados

 [ Certificados](../Sprint_7/certificados/img/txt.txt)

# Exerc√≠cios

1. [Contador](../Sprint_7/exercicios/Contador/contador_de_palavras.py)

2. [TMDB](../Sprint_7/exercicios/TMDB/tmdb/tmdb.py)

3. [GLUE](../Sprint_2/exercicios/exportacao_Biblioteca/biblioteca2/query.sql)


# Desafios

[Desafio 7](../Sprint_7/Desafio/README.MD)


# Evid√™ncias

<h1>Contador </h1>


A proposta inicial do desafio inclu√≠a o uso da imagem jupyter/all-spark-notebook no Docker para criar um ambiente com Spark e Jupyter Lab pr√©-instalados. No entanto, como o tamanho da imagem Docker era grande (5.8GB), e eu queria agilizar o processo, optei por usar o Google Colab para configurar e executar o Spark.


3. Passos Realizados

3.1 Configura√ß√£o do Ambiente no Google Colab

Instala√ß√£o do PySpark:

Iniciei instalando o PySpark no ambiente do Colab com os seguintes comandos:


`!apt-get install openjdk-8-jdk-headless -qq > /dev/null`
`!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz`
`!tar xf spark-3.1.2-bin-hadoop2.7.tgz`
`!pip install -q findspark`

Configura√ß√£o das Vari√°veis de Ambiente:

Configurei o Java e o Spark para que fossem reconhecidos no Colab:

import os
`os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"`
`os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"`

Inicializa√ß√£o do Spark:

Inicializei o Spark com o findspark e criei uma sess√£o:

`import findspark`
`findspark.init()`
`from pyspark.sql import SparkSession`
`spark = SparkSession.builder.master("local[*]").appName("Colab Spark").getOrCreate()`


Fiz um teste 

`spark.range(5).show()`

Processamento do Arquivo README.md
Carregando o Arquivo:
Montei meu Google Drive no Colab para acessar o arquivo README.md:

from google.colab import drive
drive.mount('/content/drive')
rdd = spark.sparkContext.textFile("/content/drive/MyDrive/Colab Notebooks/README.md")

(**Nota:Peguei qualquer README do meu git**)

Contagem de Palavras:
Realizei o processamento para contar as palavras no arquivo. As etapas principais foram:

Dividir o texto em palavras.
Remover palavras vazias.
Contar as ocorr√™ncias de cada palavra.

`import re`
`word_counts = (rdd.flatMap(lambda line: re.split(r'\W+', line))  `
                  ` .filter(lambda word: len(word) > 3)  `
                  ` .map(lambda word: (word.lower(), 1))  `
                  ` .reduceByKey(lambda a, b: a + b))  `

Exibi√ß√£o dos Resultados:
Transformei os resultados em um DataFrame e exibi as palavras mais frequentes:

`word_counts_df = word_counts.toDF(["Palavra", "Contagem"]).orderBy("Contagem", ascending=False)`
`word_counts_df.show(truncate=False)`

 Resultados Obtidos:
O Spark foi configurado com sucesso no Google Colab.
O arquivo README.md foi processado, e consegui identificar as palavras mais frequentes no texto.
A an√°lise revelou a contagem precisa de cada palavra, com a possibilidade de expandir para remover pontua√ß√µes e orden√°-las.

(**Nota: Considerei palavra algo com mais de 3 letras**)
![img_evidencias Colab](../Sprint_7/evidencias/ex_1_colab/Codigo1.png)

![img_evidencias Colab](../Sprint_7/evidencias/ex_1_colab/Codigo2.png)

![img_evidencias Colab](../Sprint_7/evidencias/ex_1_colab/Codigo3.png)

<h1> TMDB</h1>

Passo 1: Criando uma Conta no TMDB

N√£o vou comentar o obivo 

mas fiz a conta e Com a chave de API em m√£os, pude prosseguir com os pr√≥ximos passos.

    Configurando o Ambiente

Agora que eu tinha a chave de API, a primeira coisa que fiz foi configurar o ambiente de desenvolvimento.

Criei um arquivo .env no qual armazenei a chave de API para manter minhas credenciais seguras e evitar que fossem expostas 

diretamente no c√≥digo.

Instalei as bibliotecas necess√°rias para o projeto:

requests: para fazer requisi√ß√µes HTTP √† API.

pandas: para manipula√ß√£o e exibi√ß√£o dos dados.

python-dotenv: para carregar vari√°veis de ambiente a partir do arquivo .env.


`pip install requests pandas python-dotenv`

Em seguida, escrevi o c√≥digo Python para fazer uma requisi√ß√£o √† API do TMDB e processar os dados dos filmes.


Carregamento da Chave da API: Usei o dotenv para carregar a chave da API a partir do arquivo .env:

`from dotenv import load_dotenv`
`import os`

`# Carregar vari√°veis de ambiente`
`load_dotenv()`

`api_key = os.getenv("API_KEY")`

Requisi√ß√£o √† API: Utilizei o requests.get() para fazer a requisi√ß√£o √† API e verifiquei o c√≥digo de status da resposta para garantir que a requisi√ß√£o foi bem-sucedida:

Processamento dos Dados: Ap√≥s obter a resposta da API, extra√≠ as informa√ß√µes relevantes de cada filme, como t√≠tulo, data de lan√ßamento, vis√£o geral, votos e m√©dia de votos. Guardei esses dados em um DataFrame do Pandas para exibi-los de forma estruturada:

Exibi√ß√£o dos Dados: Por fim, utilizei o Pandas para organizar os dados em um DataFrame e exibi-los com a fun√ß√£o display():


Resultado
Ao executar o c√≥digo, consegui obter os filmes mais bem avaliados, com suas respectivas informa√ß√µes de t√≠tulo, data de lan√ßamento, vis√£o geral, votos e m√©dia de votos, todos organizados de maneira clara e f√°cil de visualizar.

![evidencias](evidencias/ex_2_TMDB/Resultado.png)

Verificando o C√≥digo de Status e Tratamento de Erros

Adicionei verifica√ß√µes adicionais no c√≥digo para garantir que o processo n√£o falhasse caso houvesse problemas na requisi√ß√£o √† API. Se o c√≥digo de status retornado n√£o for 200 (sucesso), o programa exibe uma mensagem de erro detalhada.

Neste exerc√≠cio, consegui integrar a API do TMDB ao meu c√≥digo Python com sucesso. O uso do arquivo .env para armazenar a chave de API foi uma boa pr√°tica para manter as credenciais seguras. O c√≥digo foi capaz de extrair dados dos filmes mais bem avaliados e exibi-los de forma estruturada com a ajuda do Pandas.

Codigo completo

[TMDB](../Sprint_7/exercicios/TMDB/tmdb/tmdb.py)



<h1>GLUE </h1>


Preparando os dados de origem
Primeiro, vou garantir que o arquivo nomes.csv, que cont√©m dados sobre nomes de registro de nascimento dos cart√≥rios americanos, esteja armazenado em um bucket no S3. O caminho do arquivo no S3 ser√° s3://{BUCKET}/lab-glue/input/nomes.csv. O {BUCKET} deve ser substitu√≠do pelo nome do meu bucket na conta AWS.


![alt text](evidencias/ex_3_GLUE/img_0.png)

2. Criando a IAM Role para os jobs do AWS Glue

Agora, vou criar uma IAM Role para que o AWS Glue possa executar jobs com as permiss√µes necess√°rias:

Acesso ao console do IAM (Identity and Access Management) e crio uma nova Role chamada AWSGlueServiceRole-Lab4.

![alt text](evidencias/ex_3_GLUE/img_1.png)

Na cria√ß√£o, seleciono as policies necess√°rias:

AmazonS3FullAccess

AWSLakeFormationDataAdmin

AWSGlueConsoleFullAccess

CloudWatchFullAccess

Ap√≥s isso, finalizo a cria√ß√£o da Role.

3. Configurando a conta para usar o AWS Glue

Acessarei o console do AWS Glue e, na op√ß√£o "Set up roles and users", selecionarei a Role AWSGlueServiceRole-Lab4 para garantir que o Glue tenha acesso necess√°rio.
Tamb√©m concedo permiss√µes totais de leitura e escrita para o S3.
Finalizo o processo clicando em "Next" e depois "Apply changes".

4. Configura√ß√£o do AWS Lake Formation
Vou configurar o AWS Lake Formation para criar um banco de dados no qual o crawler do Glue ir√° adicionar uma tabela automaticamente:



No console do Lake Formation, clico em "Add myself" e "Get started".
Crio um banco de dados chamado glue-lab no cat√°logo do Glue.

![alt text](evidencias/ex_3_GLUE/img_2.png)

Finalizo a cria√ß√£o do banco.


5. Criando um Job no AWS Glue
Agora, vou criar um Job ETL no Glue:

Acessarei o console do Glue, e em "ETL jobs", escolho a op√ß√£o "Visual ETL" e depois "Script editor".

Selecionei o tipo de engine "Spark" e criei o script.

Na parte "Job details", configurarei as propriedades do job, incluindo:
Name: job_aws_glue_lab_4
IAM Role: AWSGlueServiceRole-Lab4
Engine: Spark
Glue version: Glue 3.0
Language: Python 3
Worker type: G 1x
Requested number of workers: 2
Job timeout: 5 minutos
Finalmente, crio o job.

6. Desenvolvendo o c√≥digo ETL no Job
Com o job criado, vou escrever o c√≥digo para processar o arquivo nomes.csv:

Vou ler o arquivo do S3, filtrar os dados para o ano de 1934 e armazen√°-los em formato Parquet em outro local no S3.

Vou utilizar par√¢metros como S3_INPUT_PATH e S3_TARGET_PATH para tornar o c√≥digo flex√≠vel.

![alt text](evidencias/ex_3_GLUE/img_4.png)

O c√≥digo a ser implementado usar√° objetos dynamic_frames e dataframes do Glue para manipular os dados.


7. Imprimindo e manipulando os dados
Vou seguir as instru√ß√µes para manipular e imprimir dados:

Vou imprimir o esquema do dataframe gerado.
Alterar a caixa dos valores da coluna nome para MAI√öSCULO.
Imprimir a contagem de linhas no dataframe.
Agrupar os dados por ano e sexo e ordenar para mostrar o ano mais recente.
Encontrar o nome feminino e masculino mais registrados, e os anos correspondentes.

```sql
df = spark.read.csv(input_path, header=True, inferSchema=True)

# 2. Imprimir o schema do DataFrame
print("Schema do DataFrame:")
df.printSchema()

# 3. Alterar os valores da coluna nome para MAI√öSCULO
df_upper = df.withColumn("nome", upper(col("nome")))

# 4. Imprimir a contagem de linhas presentes no DataFrame
print(f"Total de linhas no DataFrame: {df_upper.count()}")

# 5. Contar nomes agrupados por ano e sexo
df_grouped = df_upper.groupBy("ano", "sexo").agg(count("nome").alias("total_nomes"))
print("Contagem de nomes agrupados por ano e sexo:")
df_grouped.show()

# 6. Ordenar os dados pelo ano mais recente
df_sorted = df_upper.orderBy(col("ano").desc())
print("Dados ordenados pelo ano mais recente:")
df_sorted.show()

# 7. Nome feminino com mais registros e o ano
df_fem = df_upper.filter(col("sexo") == "F")
most_common_fem = df_fem.groupBy("ano", "nome").agg(count("*").alias("total")) \
    .orderBy(desc("total")).first()
print(f"Nome feminino mais registrado: {most_common_fem['nome']} em {most_common_fem['ano']}")

# 8. Nome masculino com mais registros e o ano
df_masc = df_upper.filter(col("sexo") == "M")
most_common_masc = df_masc.groupBy("ano", "nome").agg(count("*").alias("total")) \
    .orderBy(desc("total")).first()
print(f"Nome masculino mais registrado: {most_common_masc['nome']} em {most_common_masc['ano']}")

# 9. Total de registros masculinos e femininos para cada ano (primeiras 10 linhas)
df_year_totals = df_upper.groupBy("ano", "sexo").agg(count("*").alias("total")) \
    .orderBy("ano").limit(10)
print("Total de registros masculinos e femininos para os primeiros 10 anos:")
df_year_totals.show()

# 10. Escrever o DataFrame com nomes em mai√∫sculo no S3
df_upper.write.mode("overwrite").partitionBy("sexo", "ano").json(output_path)

 Finalizar o job
job.commit()
```
confirma√ß√£o do resultado 

[text](exercicios/GLUE/glue.py)

![alt text](evidencias/ex_3_GLUE/img_5.png)

8. Gravando os resultados no S3
O pr√≥ximo passo √© gravar os resultados no S3:

O dataframe com os nomes em mai√∫sculo ser√° salvo no subdiret√≥rio frequencia_registro_nomes_eua no caminho s3://lab-glue/.
Vou salvar os dados em formato JSON e particionados pelas colunas sexo e ano.

![alt text](evidencias/ex_3_GLUE/img_6.png)


9. Criando o Crawler

Vou criar um Crawler para monitorar o diret√≥rio no S3 e automaticamente criar uma tabela no cat√°logo do Glue:

No console do Glue, vou acessar "Crawlers" e clicar em "Create".
Definirei o nome do Crawler como FrequenciaRegistroNomesCrawler e especificarei o caminho do S3 onde os dados est√£o.
Configurarei o Crawler para usar a Role AWSGlueServiceRole-Lab4 e definir o banco de dados de destino como glue-lab.
Finalmente, vou executar o Crawler e verificar se a tabela foi criada com sucesso no cat√°logo do Glue.

![alt text](evidencias/ex_3_GLUE/img_7.png)

10. Verificando os resultados
Ap√≥s a execu√ß√£o do Crawler, vou verificar a cria√ß√£o da tabela frequencia_registro_nomes_eua:

No Glue, vou acessar a se√ß√£o de "Tables" para confirmar que a tabela foi criada.

![alt text](evidencias/ex_3_GLUE/img_8.png)

vou realizar  consulta no Athena

![alt text](evidencias/ex_3_GLUE/img_16.png)

![alt text](evidencias/ex_3_GLUE/img_17.png)

![alt text](evidencias/ex_3_GLUE/img_18.png)

![alt text](evidencias/ex_3_GLUE/img_19.png)

![alt text](evidencias/ex_3_GLUE/img_20.png)

vou realizar algumas mais consulta no Athena para verificar os dados.

![alt text](evidencias/ex_3_GLUE/img_9.png)

![alt text](evidencias/ex_3_GLUE/img_11.png)

![alt text](evidencias/ex_3_GLUE/img_12.png)

![alt text](evidencias/ex_3_GLUE/img_13.png)

![alt text](evidencias/ex_3_GLUE/img_14.png)

![alt text](evidencias/ex_3_GLUE/img_15.png)


# Feedback

Eu gostei muito, de verdade, dos exerc√≠cios! Acho a din√¢mica extremamente interessante e percebo que aprendo muito mais dessa forma do que assistindo a v√≠deos. √â, sem d√∫vidas, uma maneira muito eficiente de ensinar.

O √∫nico ponto negativo que gostaria de destacar √© o PDF desatualizado, que acabou atrapalhando mais do que ajudando em alguns momentos.

Sobre o desafio, n√£o tive problemas para conclu√≠-lo. Confesso que me sinto um pouco inseguro pois notei que as pessoas fizeram de um jeito diferente do meu usando docker e criando mais uma camada no s3 no mesmo "nivel" da raw , mas acredito que meio jeito esteja certo tambem.