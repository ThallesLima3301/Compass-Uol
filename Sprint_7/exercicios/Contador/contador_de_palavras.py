# -*- coding: utf-8 -*-
"""Contador de Palavras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10UHv9VOEjrXKUHzes3XZoHrkRCBzfQM6
"""


!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz
!tar xf spark-3.1.2-bin-hadoop2.7.tgz
!pip install -q findspark

import os

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.2-bin-hadoop2.7"

import findspark
findspark.init()

from pyspark.sql import SparkSession

spark = SparkSession.builder.master("local[*]").appName("Colab Spark").getOrCreate()

spark.range(5).show()

from google.colab import drive
drive.mount('/content/drive')

rdd = spark.sparkContext.textFile("/content/drive/MyDrive/Colab Notebooks/README.md")

word_counts = (rdd.flatMap(lambda line: line.split(" "))
                      .filter(lambda word: word)
                      .map(lambda word: (word.lower(), 1))
                      .reduceByKey(lambda a, b: a + b))

# E
word_counts_df = word_counts.toDF(["Palavra", "Contagem"])
word_counts_df.show(truncate=False)

import re

word_counts = (rdd.flatMap(lambda line: re.split(r'\W+', line))
                   .filter(lambda word: len(word) > 3)
                   .map(lambda word: (word.lower(), 1))
                   .reduceByKey(lambda a, b: a + b))

word_counts_df = word_counts.toDF(["Palavra", "Contagem"]).orderBy("Contagem", ascending=False)
word_counts_df.show(truncate=False)