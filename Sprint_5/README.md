
Durante a Sprint 5, aprendi bastante, especialmente sobre AWS, uma √°rea que me interessou muito. O ponto mais positivo foi a introdu√ß√£o pr√°tica aos servi√ßos da AWS, que me permitiu aprender de forma simples e envolvente. Outro destaque foi a oportunidade de aprender a fazer a conex√£o com o S3 por meio do desafio proposto, o que foi uma experi√™ncia valiosa.

De forma geral, considerei a sprint f√°cil e, como terminei as atividades com bastante tempo livre, acabei explorando mais do que o necess√°rio. Aproveitei para pesquisar poss√≠veis b√¥nus e aprimorar ainda mais meus conhecimentos.
e fiz um curso sobre AWS chamado "Master AWS with Python and Bot03" achei bem legal ele
Atualmente, j√° conclu√≠ os  cursos e estou gostando bastante da trilha. Estou ansioso pela pr√≥xima sprint e pelos novos aprendizados que ela trar√° üòä.

# Certificados

 [ Certificados](../Sprint_5/certificados/img/AWS%20Skill%20Builder%20Course%20Completion%20Certificate.pdf)

  [ Link publico](https://www.credly.com/badges/8f42540e-33ce-4e93-b419-5f825c0b4111/public_url)

  
 [ Certificados](../Sprint_5/certificados/img/aws-cloud-quest-cloud-practitioner.png)

# Exerc√≠cios

1. [Resposta Ex1](../Sprint_5/exercicios/Ex.py)

Bom exercicio eu gostei de fazer de certo modo aqui eu vou documentar um pouco melhor sobre


# Desafios

[Desafio 1](../Sprint_5/Desafio/README.md)

# Evid√™ncias

Vou documentar aqui a l√≥gica por tr√°s das minhas ideas, usando imagens. A ideia √© mostrar de forma clara e direta o que eu fiz e por que tomei certas decis√µes, facilitando o entendimento do processo.

Procurei um arquivo CSV ou JSON no portal de dados p√∫blicos do Governo Brasileiro e verifiquei se ele est√° dentro do limite permitido no S3.
link do arquivo:

https://dados.gov.br/dados/conjuntos-dados/inabilitados-para-funcao-publica-segundo-tcu

![Img 1 EX](../Sprint_5/evidencias/img_resposta/img_resposta16.png)

Vou explicar um pouco sobre o meu diret√≥rio:

__pycache__:

√à o diret√≥rio onde o Python armazena os arquivos compilados (.pyc) dos m√≥dulos Python. Nesse caso, ele cont√©m os arquivos compilados data_processing e s3_operations.

Arquivos .py:
data_processing.py: Este √© o script onde voc√™ provavelmente definiu fun√ß√µes para processar o dataframe, como limpar, manipular ou analisar dados.
main.py: Este script parece ser o arquivo principal do seu projeto, onde voc√™ pode estar coordenando as opera√ß√µes entre os outros m√≥dulos e scripts.
s3_operations.py: Este script deve conter fun√ß√µes relacionadas √†s opera√ß√µes no S3, como download e upload de arquivos usando boto3.
Arquivos .csv:

relatorio_inabilitado_s3.csv: Parece ser um arquivo que voc√™ baixou do S3 ou est√° manipulando em seu projeto.
resultado_sprint_5.csv: Este provavelmente √© o arquivo resultante do processamento realizado pelo seu script, talvez ap√≥s aplicar transforma√ß√µes ou agrega√ß√µes com o Pandas.
.env:

Este √© o arquivo onde voc√™ deve estar armazenando vari√°veis de ambiente, como credenciais ou configura√ß√µes sens√≠veis que n√£o deseja expor diretamente no c√≥digo. O arquivo .env √© usado junto com a biblioteca dotenv para carregar essas vari√°veis de forma segura.

Agora vou explicar um pouco da minha l√≥gica. A ideia de usar o arquivo .env surgiu porque eu n√£o gostava de ver as senhas aparecendo diretamente no meu c√≥digo. Ent√£o, pesquisei uma forma de escond√™-las e proteger essas informa√ß√µes.

Eu importei o m√≥dulo logging porque pesquisei sobre ele e achei interessante adicion√°-lo ao meu c√≥digo como um b√¥nus. √â uma forma de registrar eventos e informa√ß√µes √∫teis, ajudando a monitorar o comportamento do programa e identificar problemas de maneira mais profissional.

Vamos fala do desafio. 

# Documenta√ß√£o do Desafio AWS S3

1. Objetivo
Meu objetivo com esse desafio foi praticar os conhecimentos adquiridos sobre AWS, especificamente com o servi√ßo S3. A ideia era manipular arquivos diretamente no S3 usando scripts em Python com a biblioteca boto3 e pandas.

2. Entreg√°veis
Para concluir o desafio, preparei os seguintes entreg√°veis:

Um arquivo Markdown com evid√™ncias (prints) e documenta√ß√£o explicando cada etapa executada.
Arquivos .py contendo os c√≥digos para download, processamento, e upload dos arquivos no S3.
Um arquivo CSV com os resultados processados e salvos.
3. Prepara√ß√£o
Primeiramente, certifiquei-me de que minha conta da Compass AWS estava configurada e que eu tinha as permiss√µes necess√°rias para criar e manipular buckets e arquivos no S3.

4. Desafio
O desafio consistiu em trabalhar com dados de um CSV baixado do portal de dados p√∫blicos do Governo Brasileiro. Escolhi o arquivo relatorio_inabilitado.csv e segui os passos abaixo:

4.1 AWS S3
1. Escolha do arquivo
Escolhi o arquivo relatorio_inabilitado.csv porque ele se encaixava nos crit√©rios exigidos pelo desafio. Garanti que ele era √∫nico na turma.

2. An√°lise dos dados
Analisei o conte√∫do do arquivo localmente usando um editor de texto para conhecer os dados e planejar as manipula√ß√µes necess√°rias.

3. Carregamento no S3
Utilizei um script Python para carregar o arquivo relatorio_inabilitado_s3.csv para um bucket no S3, utilizando a biblioteca boto3. Fiz a configura√ß√£o das credenciais no arquivo .env para manter as informa√ß√µes sens√≠veis fora do c√≥digo.

4. Manipula√ß√£o do DataFrame
Em outro script, carreguei o arquivo diretamente do S3 e criei um DataFrame usando pandas para aplicar as seguintes manipula√ß√µes:

Usei uma cl√°usula que filtra dados com dois operadores l√≥gicos: selecionei registros onde a "Data Final" era posterior a 2025 e o "Tr√¢nsito em Julgado" era anterior a 2023.
Apliquei fun√ß√µes de agrega√ß√£o, como calcular a diferen√ßa de anos entre duas colunas de data.
Utilizei uma fun√ß√£o condicional para determinar se um registro estava "V√°lido" ou "Expirado" com base na data atual.
Fiz convers√µes de colunas para strings formatadas, incluindo a limpeza de CPFs e transforma√ß√£o de nomes para mai√∫sculas.
5. Salvando e enviando o arquivo
Depois de processar o DataFrame, salvei o resultado em um arquivo CSV chamado resultado_sprint_5.csv e o enviei de volta para o bucket no S3.

6. Documenta√ß√£o no Git
Documentei todo o processo em um arquivo Markdown, incluindo prints das execu√ß√µes e os c√≥digos utilizados. Armazenei os arquivos .csv e .py, juntamente com as evid√™ncias, no meu reposit√≥rio Git.


# Feedback

De modo geral, durante essa sprint, percebi que acabei procrastinando bastante, assim como na sprint anterior. Achei as duas muito f√°ceis, o que me fez finalizar todas as atividades com anteced√™ncia, levando-me a enrolar um pouco. No entanto, gostei muito de aprender sobre AWS; at√© o momento, √© a √°rea pela qual desenvolvi mais interesse em todas as sprints.

Minha avalia√ß√£o para essa sprint √© 6/10. A experi√™ncia com o jogo de AWS foi √≥tima, achei-o f√°cil e muito intuitivo, e espero que haja mais jogos como esse no futuro. Particularmente, gostei bastante de criar a conex√£o com a AWS e espero continuar explorando essa √°rea.