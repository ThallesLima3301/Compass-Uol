
Durante a Sprint 5, aprendi bastante, especialmente sobre AWS, uma √°rea que me interessou muito. O ponto mais positivo foi a introdu√ß√£o pr√°tica aos servi√ßos da AWS, que me permitiu aprender de forma simples e envolvente. Outro destaque foi a oportunidade de aprender a fazer a conex√£o com o S3 por meio do desafio proposto, o que foi uma experi√™ncia valiosa.

De forma geral, considerei a sprint f√°cil e, como terminei as atividades com bastante tempo livre, acabei explorando mais do que o necess√°rio. Aproveitei para pesquisar poss√≠veis b√¥nus e aprimorar ainda mais meus conhecimentos.
e fiz um curso sobre AWS chamado "Master AWS with Python and Bot03" achei bem legal ele
Atualmente, j√° conclu√≠ os  cursos e estou gostando bastante da trilha. Estou ansioso pela pr√≥xima sprint e pelos novos aprendizados que ela trar√° üòä.

# Certificados

 [ Certificados](../Sprint_5/certificados/img/AWS%20Skill%20Builder%20Course%20Completion%20Certificate.pdf)

  [ Link publico](https://www.credly.com/badges/8f42540e-33ce-4e93-b419-5f825c0b4111/public_url)

  
 [ Certificados](../Sprint_5/certificados/img/aws-cloud-quest-cloud-practitioner.png)

# Exerc√≠cios

1. [Resposta Ex1](../Sprint_5/exercicios/index.html)

Documenta√ß√£o do Exerc√≠cio de Hospedagem de Site Est√°tico no AWS S3
Bom exerc√≠cio, eu gostei de fazer, de certo modo. Aqui eu vou documentar um pouco melhor sobre. Achei o enunciado muito grande para coisas algo t√£o simples.

Passo 1: Criar um Bucket no S3
O primeiro passo foi acessar o console do AWS S3 e criar um bucket para hospedar o site. Defini o nome e a regi√£o do bucket, escolhendo us-east-1 para facilitar a configura√ß√£o e deixar no padr√£o.

![Img 17 EX](../Sprint_5/evidencias/img_resposta/img_resposta17.png)


Passo 2: Habilitar a Hospedagem Est√°tica
Depois de criar o bucket, eu habilitei a hospedagem de site est√°tico. Defini o index.html como o documento de √≠ndice e o 404.html como o documento de erro. Foi simples, mas achei que poderia ser mais direto no enunciado.


Passo 3: Upload dos Arquivos
Fiz o upload dos arquivos necess√°rios:

![Img 18 EX](../Sprint_5/evidencias/img_resposta/img_resposta18.png)

index.html: P√°gina inicial do site com um link para download do CSV.
404.html: P√°gina de erro personalizada.
nomes.csv: O arquivo de dados que ser√° baixado pelo usu√°rio.

Passo 4: Configurar Permiss√µes e Pol√≠tica de Bucket
Para garantir que o site funcionasse, precisei ajustar as permiss√µes:

![Img 19 EX](../Sprint_5/evidencias/img_resposta/img_resposta19.png)

Editei a Pol√≠tica do Bucket para permitir acesso p√∫blico de leitura a todos os objetos.
Verifiquei as permiss√µes individuais de cada arquivo, garantindo que todos tivessem a leitura p√∫blica habilitada.

Passo 5: Testar o Endpoint do Site
Com tudo configurado, testei o endpoint do site. O index.html carregou corretamente, e o link para download do CSV estava funcionando ap√≥s ajustar as permiss√µes.

![Img 20 EX](../Sprint_5/evidencias/img_resposta/img_resposta20.png)

![Img 20 EX](../Sprint_5/evidencias/img_resposta/img_resposta21.png)

Conclus√£o
Achei interessante explorar o AWS S3 para hospedar um site est√°tico, mas percebi que, apesar de ser um processo simples, o enunciado poderia ser mais objetivo. No geral, foi um bom exerc√≠cio para entender as configura√ß√µes b√°sicas de permiss√µes e pol√≠ticas no S3.



# Desafios

[Desafio 1](../Sprint_5/Desafio/README.md)

# Evid√™ncias

Vou documentar aqui a l√≥gica por tr√°s das minhas ideas, usando imagens. A ideia √© mostrar de forma clara e direta o que eu fiz e por que tomei certas decis√µes, facilitando o entendimento do processo.

Procurei um arquivo CSV ou JSON no portal de dados p√∫blicos do Governo Brasileiro e verifiquei se ele est√° dentro do limite permitido no S3.
link do arquivo:

https://dados.gov.br/dados/conjuntos-dados/inabilitados-para-funcao-publica-segundo-tcu

![Img 1 EX](../Sprint_5/evidencias/img_resposta/img_resposta16.png)

Vou explicar um pouco sobre o meu diret√≥rio:

__pycache__:



No meu projeto, o diret√≥rio __pycache__ √© onde o Python armazena os arquivos compilados (.pyc) dos m√≥dulos que utilizo. No meu caso, ele cont√©m os arquivos compilados dos scripts data_processing e s3_operations.

Arquivos .py:
data_processing.py: Este √© o script onde defini as fun√ß√µes para processar o DataFrame, incluindo tarefas como limpar, manipular e analisar dados.
main.py: Esse √© o arquivo principal do meu projeto, onde coordeno as opera√ß√µes entre os diferentes m√≥dulos e scripts que criei.
s3_operations.py: Esse script cont√©m fun√ß√µes espec√≠ficas para opera√ß√µes no S3, como download e upload de arquivos usando a biblioteca boto3.
Arquivos .csv:
relatorio_inabilitado_s3.csv: Este √© o arquivo que baixei do S3 e que utilizo para manipula√ß√µes e an√°lises no meu projeto.
resultado_sprint_5.csv: Este √© o arquivo gerado ap√≥s o processamento realizado pelo meu script, onde aplico transforma√ß√µes e agrega√ß√µes com o Pandas.
.env:
O arquivo .env √© onde armazeno minhas vari√°veis de ambiente, como credenciais e configura√ß√µes sens√≠veis que n√£o quero expor diretamente no c√≥digo. Utilizo a biblioteca dotenv para carregar essas vari√°veis de maneira segura.

Eu decidi usar o arquivo .env porque n√£o gostava de ver as senhas aparecendo diretamente no c√≥digo. Por isso, pesquisei uma maneira de escond√™-las e proteger essas informa√ß√µes, e encontrei essa solu√ß√£o.

Al√©m disso, importei o m√≥dulo logging ap√≥s fazer uma pesquisa sobre ele e achei interessante adicion√°-lo ao meu c√≥digo como um b√¥nus. Ele √© uma forma eficiente de registrar eventos e informa√ß√µes √∫teis, o que ajuda a monitorar o comportamento do programa e a identificar problemas de maneira mais profissional.
Vamos fala do desafio. 

# Documenta√ß√£o do Desafio AWS S3

1. Objetivo
Meu objetivo com esse desafio foi praticar os conhecimentos adquiridos sobre AWS, especificamente com o servi√ßo S3. A ideia era manipular arquivos diretamente no S3 usando scripts em Python com a biblioteca boto3 e pandas.

2. Entreg√°veis
Para concluir o desafio, preparei os seguintes entreg√°veis:

Um arquivo Markdown com evid√™ncias (prints) e documenta√ß√£o explicando cada etapa executada.
Arquivos .py contendo os c√≥digos para download, processamento, e upload dos arquivos no S3.
Um arquivo CSV com os resultados processados e salvos.
3. Prepara√ß√£o
Primeiramente, certifiquei-me de que minha conta da Compass AWS estava configurada e que eu tinha as permiss√µes necess√°rias para criar e manipular buckets e arquivos no S3.

4. Desafio
O desafio consistiu em trabalhar com dados de um CSV baixado do portal de dados p√∫blicos do Governo Brasileiro. Escolhi o arquivo relatorio_inabilitado.csv e segui os passos abaixo:

4.1 AWS S3
1. Escolha do arquivo
Escolhi o arquivo relatorio_inabilitado.csv porque ele se encaixava nos crit√©rios exigidos pelo desafio. Garanti que ele era √∫nico na turma.

2. An√°lise dos dados
Analisei o conte√∫do do arquivo localmente usando um editor de texto para conhecer os dados e planejar as manipula√ß√µes necess√°rias.



3. Carregamento no S3
Utilizei um script Python para carregar o arquivo relatorio_inabilitado_s3.csv para um bucket no S3, utilizando a biblioteca boto3. Fiz a configura√ß√£o das credenciais no arquivo .env para manter as informa√ß√µes sens√≠veis fora do c√≥digo.

4. Manipula√ß√£o do DataFrame
Em outro script, carreguei o arquivo diretamente do S3 e criei um DataFrame usando pandas para aplicar as seguintes manipula√ß√µes:

![Img 1 EX](../Sprint_5/evidencias/img_resposta/img_resposta7.png)

Usei uma cl√°usula que filtra dados com dois operadores l√≥gicos: selecionei registros onde a "Data Final" era posterior a 2025 e o "Tr√¢nsito em Julgado" era anterior a 2023.

![Img 9 EX](../Sprint_5/evidencias/img_resposta/img_resposta9.png)

Apliquei fun√ß√µes de agrega√ß√£o, como calcular a diferen√ßa de anos entre duas colunas de data.

![Img 10 EX](../Sprint_5/evidencias/img_resposta/img_resposta10.png)

Utilizei uma fun√ß√£o condicional para determinar se um registro estava "V√°lido" ou "Expirado" com base na data atual.

![Img 11 EX](../Sprint_5/evidencias/img_resposta/img_resposta11.png)

Fiz convers√µes de colunas para strings formatadas, incluindo a limpeza de CPFs e transforma√ß√£o de nomes para mai√∫sculas.

![Img 12 EX](../Sprint_5/evidencias/img_resposta/img_resposta12.png)

Fiz algumas mudan√ßas no meu c√≥digo para deix√°-lo mais pr√°tico e organizado. Primeiro, adicionei um comando que cria a pasta resultados automaticamente, garantindo que os arquivos CSV sejam salvos sem erro, independente de onde o script rode. Tamb√©m usei vari√°veis para deixar mais f√°cil mudar o nome dos arquivos no futuro, sem precisar mexer em v√°rias partes do c√≥digo.

Al√©m disso, troquei o m√©todo .head() pelo .sample() para mostrar amostras aleat√≥rias dos dados(somente para o video), dando uma vis√£o mais variada dos registros. Estruturei o upload dos arquivos para o S3 de forma mais organizada, colocando tudo em pastas. E, por fim, implementei o logging para registrar mensagens e monitorar a execu√ß√£o, facilitando a identifica√ß√£o de problemas.

Essas mudan√ßas tornaram o c√≥digo mais flex√≠vel e f√°cil de ajustar para diferentes cen√°rios, deixando o fluxo mais automatizado e eficiente.

![Img 9 EX](../Sprint_5/evidencias/img_resposta/img_resposta24.png)

5. Salvando e enviando o arquivo

    # Fazer o upload dos arquivos CSV para o bucket S3
    upload_file_s3(bucket_name, 'resultados/tabela_original.csv', 'resultados/tabela_original.csv')
    upload_file_s3(bucket_name, 'resultados/tabela_filtrada.csv', 'resultados/tabela_filtrada.csv')
    upload_file_s3(bucket_name, 'resultados/tabela_expirados.csv', 'resultados/tabela_expirados.csv')
    upload_file_s3(bucket_name, output_file, output_file)

6. Documenta√ß√£o no Git
Documentei todo o processo em um arquivo Markdown, incluindo prints das execu√ß√µes e os c√≥digos utilizados. Armazenei os arquivos .csv e .py, juntamente com as evid√™ncias, no meu reposit√≥rio Git.


# Feedback

De modo geral, durante essa sprint, percebi que acabei procrastinando bastante, assim como na sprint anterior. Achei as duas muito f√°ceis, o que me fez finalizar todas as atividades com anteced√™ncia, levando-me a enrolar um pouco. No entanto, gostei muito de aprender sobre AWS; at√© o momento, √© a √°rea pela qual desenvolvi mais interesse em todas as sprints.

Minha avalia√ß√£o para essa sprint √© 6/10. A experi√™ncia com o jogo de AWS foi √≥tima, achei-o f√°cil e muito intuitivo, e espero que haja mais jogos como esse no futuro. Particularmente, gostei bastante de criar a conex√£o com a AWS e espero continuar explorando essa √°rea.